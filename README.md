# PARCO-Computing-2026-243669 
## Table of Contents

1. [Project Overview](#1-project-overview)

2. [Repo layout](#2-repo-layout)

3. [Compilation](#3-compilation)

4. [Running the Code](#4-running-the-code)

5. [Input Data](#5-input-data)

6. [Modifying Parameters](#6-modifying-parameters)

7. [Cluster Notes](#7-cluster-notes)

8. [Reproducibility and Data](#8-reproducibility-and-data)

  
## 1. Project Overview

Author: Riccardo, Speretta 243669 riccardo.speretta@studenti.unitn.it
This project constitutes the final project for the Parallel Computing 2025/2026 course at unitn.

---
## 2. Repo layout

```bash

├── README.md

├── Makefile

├── src/        # C/C++ source code

├── generator/  # code from Graph500 for R-MAT graph generation

├── scripts/    # run/plot scripts

├── results/    # BFS runs results

├── plots/      # figures for the report

└── data/      # input graphs

```

#### src
- `BFS_basic.c`
  = algorithm for plain MPI distributed BFS implementation

- `BFS_hybrid.c`
  = algorithm for hybrid MPI+OpenMP distributed BFS implementation
  (poor modularity between the two programs can be stressing for the reader, but they are basically a very similar variation of the same program)

- `csrMaker.c`
  = responsible for building the correct CSR convertion of the txt edge list of the graph provided in data/raw, and placing it into data/csr

- `graph_utils.h` / `graph_utils.c`
  = helper functions for the previous programs, including a parallel sorting, the writing of a BFS runs into its result file and the validate function (a simplified version of the Graph500 one)

- `bitset.h` / `bitset.c`
  = custom bitsets used for visited vertices list and frontier in both BFS algorithms, including atomic functionality

#### generator
Directly coming from the Graph500 open source implementation: https://github.com/graph500/graph500
in `generator.c` a small main has been added in order to generate the desired graphs

#### scripts
*keep in mind*: ALL scripts and executables are expected to be run inside the root folder of the project. 

- `SNAP_list.sh`
    = simple list of SNAP graphs that one intends downloading, this list maps if a graph is already undirected or not

- `setup.pbs`
    = job intended to be submitted first, in order to prepare the correct ambient for the execution of the BFS algorithms

- `build_list.sh`
    = calls the `csrMaker.out` executable on all .txt files that need to be translated into CSR

- `basic_benchmark.pbs`
    = allocates resources and call for the `BFS_basic.out` executable over a growing amount of cores, correctly adapting to real-world graphs and generated onese (results are directed to the correct folder), and already processed graps are not processed again

- `hybrid_benchmark.pbs`
    = same thing of the previous script but for `BFS_hybrid.out`

- `clear_results.sh`
    = quick way to erase contents in the `results` folder, accept parameters such as `basic` or `hybrid` for selected delition


each script will provide helpful prints to allow the user understand the phase reached by the scripts in their execution.
  
#### scripts/plotting
*keep in mind*: ALL scripts and executables are expected to be run inside the root folder of the project. 

nested folder dedicated to python scripts responsible for plotting:
- `loaders.py` 
  = reads data from the `results/` folder and produces data frames that python can work upon
- `summaries.py`
  = creates a summary of read results
- `plot_results.py`
  = actually calls different functions to create the desired graphs and stores them in `plots/`
  
#### results
(its contents, given their sizes, are not included by default in the repo, but will be populated by the previous scripts)

for each graph, there will be one directory, containing a set of txt files:
- **basic**x = pure MPI implementation runs on x cores

- **hybrid**x = hybrid MPI+OpenMP implementation runs on x cores

each "end point file" contains a list of 5 measurements as reported in the paper: time per solution (max across ranks), communication time(max across ranks), TEPS(global), max_over_mean(global), CV(global), reapeated once for each of the 64 BFS executions.

#### plots
(meant to be filled locally with the Python scripts, not part of the cluster execution)

#### data
(its contents, given their sizes, are not included by default in the repo, but will be populated by the previous scripts)
- **raw**
    = all the `graph.txt` files downloaded by SNAP or generated by `generator.out`

- **csr**
    = contains the outputs of `csrMaker.out`

  ---
## 3. Compilation
The compiler version used on the cluster for all compilations of C code is `gcc11.2.0`
While for mpicc is `OpenMPI/4.1.1-GCC-11.2.0`

The `make` command, calling the `Makefile` will be enough for compiling all necessary C programs. 
As for the scripts these executables must be run from the root folder of the project to work correctly.

A `DEBUG` flag exists for all the 3 "standalone" `.c` files in `src`, which allow for extra printings during execution for a better understanding of what's happening.

---
## 4. Running the code
Although local and individual execution of compiled C programs is possible the following pipeline allows for a coherent execution on the cluster, leaving little room for mistakes:

- after making sure that all SNAP graphs have been correctly downloaded (and unzipped) in the `data/raw` folder
- `setup.pbs`: will first generate all the Kronecker graphs as specified, starting with a scale of 22 for 8 cores, then proceding to build all CSRs needed, important also for the creation of the results folders
- `basic_benchmark.pbs`: runs the pure MPI algorithm on all of the found CSR graphs 
- `hybrid_benchmark.pbs`: runs the pure MPI+OpenMP algorithm on all of the found CSR graphs 


---
## 5. Modifying Parameters
Beside from what gets iteratively and adabtably changed inside the scripts
- `SNAP_list.sh` allows for a pre-selection for specifying if the graph from SNAP is directed or not (needed for translation into undirected file in `build_list.sh`)
- also, in `build_list.sh` is possible is possible to specify if graphs' IDs should be shuffled by `csrMaker.out` or not
- `generator.out` can accept different scales and edgefactors, but in `setup.pbs` we already select an edgefactor of 16 (Graph500 choice) and reach up to a scale of 26, making the graph already significantly large for the cluster capabilities and the scope of this project

---
## 6. Cluster Notes
Cluster-specific notes (modules, queues)
- because of the inevitable differences among possible nodes assigned to the job across different runs the pbs scripts avoid re-processing the same graphs twice, so multiple runs can be compatible. Meaning that the execution of such scripts doesn't care particularly for the selection of specific hardware inside the cluster (choice based on time and cluster limitations, forcing queues to take too much time)
  
---
## 7. Reproducibility and Data
- For full transparency the exact results used in the paper are also reported in the `paper_data` folder 
  