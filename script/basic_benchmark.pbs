#!/bin/bash
# Job name
#PBS -N hybridBFS
# Output files
#PBS -o ./hybrid.o
#PBS -e ./hybrid.e
# Queue name
#PBS -q shortCPUQ
# Set the maximum wall time
#PBS -l walltime=0:40:00
# Number of nodes, cpus, mpi processors and amount of memory -> allowing for each MPI process having 8 cores for OpenMP threads parallelization
#PBS -l select=4:ncpus=32:mpiprocs=32:mem=40gb
#PBS -l place=scatter

# Modules for python and MPI
module load GCC/11.2.0
module load OpenMPI/4.1.1-GCC-11.2.0

gcc --version
mpicc --version

# Get the list of unique nodes assigned to the job
NODES=$(sort -u $PBS_NODEFILE)
echo The running nodes are $NODES

# Loop through each node and get architecture information
for NODE in $NODES; do
    echo "Node: $NODE"
    ssh $NODE "lscpu"
done

# Select the working directory 
cd $PBS_O_WORKDIR

# the code should be previously compiled

# STRONG SCALING
# for each graph (in CSR format)
for graph in data/csr/*.bin; do
    # ignore R-MAT for now
    if [[ "${graph}" == data/csr/kronecker* ]]; then
        continue
    done

    # going from 1 to 128 MPI processes
    echo "[ BENCHMARKING ${graph} with basic BFS ]"
    for np in 1 2 4 8 16 32 64 128; do
        echo "Running with $np processes"
        # for 64 different (randomized) search keys (repetitions inside the program)
        mpirun -np $np --hostfile $PBS_NODEFILE --report-bindings --map-by node --bind-to core ./BFS_basic.out ${graph} basic${np} 
        echo "$np run is OVER ========"
        echo ""
    done
    echo "[ DONE ${graph} ]"
done

# WEAK SCALING
for graph in data/csr/kronecker*.bin; do

done